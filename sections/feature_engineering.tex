\documentclass[../diploma.tex]{subfiles}
 
 \begin{document}

\newpage
\subsection{Извлечение признаков}
В предыдущем подразделе мы уже описали пример признака, извлекаемого в качестве локального условия. Стоит упомянуть о том, что проектирование и подбор оптимального набора признаков это своего компромисс. 
Дело в том, что время обучения WaveNet растёт линейно от ширины слоёв, которые в свою очередь линейно зависят от количества признаков. Ввиду наших достаточно скромных для такой задачи как генерация голоса вычислительных ресурсов, мы лишены роскоши использования настолько большого набора признаков, насколько мы способны реализовать.
Несмотря на это, в этом разделе мы опишем признаки, которые мы посчитали полезными для повышения качества генерации, однако тем, которые использовались в качестве локальных условий, уделим отдельное внимание.

% Вторая: нужно выровнять текст вдоль звука. Тут мы тоже воспользовались самым простым подходом и просто растянули слова, дублирая буквы вдоль звукового сигнала.

% У такого подхода куча есть очевидные минусы, от одного из которых понятно, как избавиться, а от второго не очень. Дело в том, что по идее сам текст несёт очень мало информации о том, как его надо произносить. Для нейросети сырой текст это очень низкоуровневое представление с точки зрения произношения. Да и энекодер у нас, честно говоря, сомнительный. Поэтому хотелось бы передавать что-то более высокоуровевое, например, морфемы. 


\subsubsection{Признаки для представления данных}
Опишем признаки, которые извлекаются нашим скриптом из звукового файла таблицей \ref{table:voice_fearures}.

\begin{table}[!htbp]
\centering
\caption{Голосовые признаки}
\label{table:voice_fearures}
\begin{tabular}{|c|l|l|}
\hline
\textbf{Id} & \multicolumn{1}{c|}{\textbf{Признак}} & \multicolumn{1}{c|}{\textbf{Описание}}                                                                                                                                              \\ \hline
1           & Zero Crossing Rate                    & Количество смен знака сигнала в рамках временного фрейма                                                                                                                            \\ \hline
2           & Energy                                & \begin{tabular}[c]{@{}l@{}}Сумма квадратов значений сигнала, нормализованная \\ по длине  фрейма\end{tabular}                                                                       \\ \hline
3           & Entropy of Energy                     & Энтропия нормализованной энергии подфреймов                                                                                                                                        \\ \hline
4           & Spectral Centroid                     & Центр тяжести спектра                                                                                                                                                               \\ \hline
5           & Spectral Spread                       & Второй центральный момент спектра                                                                                                                                                   \\ \hline
6           & Spectral Entropy                      & \begin{tabular}[c]{@{}l@{}}Энтропия нормализованных энергий спектра для \\ набора подфреймов\end{tabular}                                                                           \\ \hline
7           & Spectral Flux                         & \begin{tabular}[c]{@{}l@{}}Квадратичное отклонение между нормализованными \\ аплитудами спектров \\ двух соседних фреймов\end{tabular}                                              \\ \hline
8           & Spectral Rolloff                      & \begin{tabular}[c]{@{}l@{}}Частота ниже которой сконцетрированны 90\% значений \\ распределения\end{tabular}                                                                        \\ \hline
9-21        & MFCCs                                 & MEL-кепcтральные коэффициенты представления                                                                                                                                         \\ \hline
22-33       & Chroma Vector                         & \begin{tabular}[c]{@{}l@{}}12-элементное представление спектральной энергии, \\ элементы которого представляют 12 равноценных\\  классов тангажа музыки западного типа\end{tabular} \\ \hline
34          & Chroma Deviation                      & Стандартное отклонение предыдущих 12 коэффициентов                                                                                                                                  \\ \hline
\end{tabular}
\end{table}

Каждый признак извлекается фреймами по 10 миллисекунд без пересечений, но также дублируется на фреймах по 100 миллисекунд c шагом 10 миллисекунд, то есть с пересечениями, причём с заглядыванием как вперёд, так и назад.

Однако такой набор признаков весьма сильно нагружает канал локального условия, заставляя сеть сходиться очень долго. Так как эксперименты показали, что сеть может генерировать приличный голос даже без них, решено было в финальном эксперименте от дополнительных признаков для представления данных отказаться.

\subsubsection{Признаки для глобального условия}
Признаки для глобального условия должны описывать некоторую характеристику, которая будет задавать особенность генерируемой моделью речи. В финальных экспериментах использовались два основных признака: пол и идентификационный номер говорящего. Для того чтобы использовать идентификационный номер на этапе генерации, нужно задать ещё на стадии обучения, сколько всего в обучающей выборке разных голосов, чтобы в реализации построить \verb|one-hot encdoing| для номеров говорящих.

\subsubsection{Признаки для локального условия}
Признаки для локального условия должны в основном опираться на текст, поскольку мы стараемся добиться, чтобы генерируемая речь была более осмыслена и в перспективе приближала модель к решению задачи трансляции текста в голос.
В таком случае стандартный word2vec автоэнкодер нам совсем не подходит, потому что последний улавливает лишь семантическую близость между словами, а мы хотели бы иметь высокоуровневое представление, содержащее больше информации о том, \textbf{как} произносить слова.

Первым приходящим на ум представлением, объединяющим в себе морфологические свойства речи были бы морфемы.
Однако вспомним, что в силу нюансов архитектуры всплывёт двойная проблема с выравниваем: выравнивание морфем вдоль текста, а потом выравнивание результатов морфологического анализа вдоль звука. 
В такой постановке извлечение признака звучит очень рискованно и трудоёмко, в итоге мы рискуем получить сильную погрешность за счёт промежуточных шагов, потратив много усилий на нетривиальную реализацию.

Альтернативным решением стало использовать в качестве признаков речь, сгенерированную открытым языковым веб-интерфейсом Yandex Speech Kit \cite{online:speech_kit}.
Для этих целей был выбран yandex-speech-kit по двум основным причинам:
\begin{enumerate}
    \item Speech-kit имеет бесплатный суточный лимит для исследовательских целей
    \item Yandex любезно согласился расширить для нас этот лимит, поскольку у нас очень много данных 
\end{enumerate}
Для генерации мы использовали голос по умолчанию Jane. Поскольку в корпусе VCTK очень часто используются одни и те же фразы, мы завели глобальную хеш-таблицу всех фраз, чтобы запоминать, для каких фраз мы уже сгенерировали речь и переиспользовать.

Такое представление достаточно высокоуровневое и компактное, что позволит нам дождаться результатов обучения.

\subsubsection{Выравнивание аудио}
Теперь стоит рассказать, как мы избавились от проблемы с выравниванием.
В общем случае выравнивание двух временных рядов на одинаковое количество фреймов это достаточно ad hoc задача, сильно зависящая от семантики извлекаемого признака. Основная проблема заключается в том, что в результате выравнивания мы теряем информацию для более длинного ряд а либо эмулируем недостающую для более короткого. В нашем случае, нашёлся удобный способ обойти эту проблему, варьируя частоту сэмплирования при чтении аудио.

Обозначим длины оригинального и сгенерированного аудио как $LEN_{orig}$ и $LEN_{gen}$ соответственно при частоте сэмплирования  =  16000 Гц. Тогда чтобы получить временные ряды одинаковой длины считаем ещё раз сгенерированное аудио отнормировав частоту на величину $\frac{LEN_{orig}}{LEN_{gen}}$. Если у нас где-то возникнет погрешность на пару фреймов из-за деления, исправим их механически, просто выкинув лишние из конца.


\end{document}

