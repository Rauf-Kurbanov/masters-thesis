\documentclass[../diploma.tex]{subfiles}
 
\begin{document}
\label{sec:existing_solutions}
   
Методология генерации голоса имеет достаточно богатую историю на протяжении большей части которой в основе самых передовых на тот момент методов лежали системы на основе скрытых марковских моделей.
Во многом благодаря тому, что стоит задача построения генеративной модели, марковские свойства до сих присутствуют, однако могут ослабляться, как, например, в нашем решении. 
Современные системы генерации голоса всё больше основываются на глубинном обучении как в качестве вспомогательного механизма, так и полной замены скрытых марковских моделей.

\subsection{Генеративные модели}
% \label{sec:goals}
\subfile{sections/generative_models}


\subsection{Генерация речи на основе скрытых марковских моделей}

\begin{definition}
\textbf{Скрытая марковская модель (СММ)} — статистическая модель, имитирующая работу процесса, похожего на марковский процесс с неизвестными параметрами, и задачей ставится разгадывание неизвестных параметров на основе наблюдаемых. Полученные параметры могут быть использованы в дальнейшем анализе, например, для распознавания образов. СММ может быть рассмотрена как простейшая байесовская сеть доверия.
    \cite{wiki:hmm}
\end{definition}

% 

     
\begin{itemize}
    \item Поменяв местами вход и выход скрытой марковской модели в задаче распознавания голоса, мы обращаем скрытую марковскую модель в генеративную модель для задачи трансляции текста в звук.
    \item Спектр речи, фундаментальная частота, озвучивание и длительность моделируются одновременно по скрытой марковской модели. \cite{yoshimura1999simultaneous}
    \item Обучение моделей и генерация сигналов происходит по универсальному критерию максимального правдоподобия.
\end{itemize}
\cite{tokuda2002hmm}

\subsubsection{Преимущества генерации на основе СММ}
\begin{itemize}
    \item Статистическая параметрическая модель может быть эффективно обучена на речевых данных в цифровом формате с соответствующими транскрипциями.
    \item Статистическая модель на основе СММ имеет достаточно малый размер и эффективна в плане использования данных.
\end{itemize}

\subsection{Генерация речи на основе глубинного обучения}
    
    Глубинное обучение оказало огромное влияние на исследования, продукты и сервисы в области  автоматического распознавания речи. Перечислим причины популярности использования глубоких нейросетевых архитектур в данной области.
    
    \begin{enumerate}
        \item Встроенное извлечение признаков.
    \end{enumerate}    
    
    Трудно спросить, что автоматическое или хотя бы частично-автоматическое извлечение признаков просто мечта для исследователя. Особенно при работе с такими сложными распределениями как изображения и звук, когда внутреннее устройство признаков в значительной мере представляет из себя чёрный ящик.
    
    % \begin{itemize}
        \begin{itemize}
            \item Возможность эффективно моделировать сильно коррелирующие признаки большой размерности. 
            \item Слоёная архитектура с нелинейными операторами позволяет бесплатно интегрировать извлечение признаков в языковую модель.
        \end{itemize}    
    % \end{itemize}
    
    \begin{enumerate}[resume]
        \item Распределённое представление.
    \end{enumerate}    
        Здесь имеется ввиду, что нейросети зачастую способные единовременно захватывать большой участок пространства данных, когда, к примеру, деревья решений дробят пространство данных на области, что приводит к высокой фрагментации. 
    
    % \begin{itemize}
        \begin{itemize}
            \item Может быть экспоненциально более эффективно чем фрагментированное представление.
            \item Лучшие описательные качества с меньшим количеством параметров.
        \end{itemize}
    % \end{itemize}
    
    \begin{enumerate}[resume]
        \item Слоёная иерархическая структура системы генерации речи.
    \end{enumerate}
    
    Глубокие нейросети неявно расслаивают промежуточные представление, при движении вглубь по слоям.
    
    % Звуковая волна $\rightarrow$ концептуальное представление $\rightarrow$ лингвистическое $\rightarrow$ представление с упором на описание методов произношения $\rightarrow$ звуковая волна
    $$
        \verb|Звуковая волна| \rightarrow \verb|концептуальное представление| \rightarrow \verb|лингвистическое представление|$$ $$\rightarrow \verb|представление с упором на описание методов произношения| \rightarrow \verb|звуковая волна|.
    $$
    \cite{pres:google_dl_speech}.
    
    Хоть целью работы и не является генерация речи напрямую, может возникнуть вопрос, почему была выбрана та ли иная подлежащая нейросетевая архитектура. Благо создатели нейросетевой архитектуры WaveNet  позиционируют её как новый state of the art, поэтомоу с любыми вопросами относительно качества генерации мы можем отправить в оригинальную статью \cite{article:van2016wavenet}.
    
    Однако, качество генерации было не единственным критерием выбора, поэтому мы опишем предыдущую "state of the art" систему и обоснуем свой выбор. 

\subsubsection{Глубинные архитектуры для генерации речи}
\begin{itemize}
    \item HMM-DBN (USTC/MSR \cite{ling2013modeling}, \cite{ling2013modeling2}).
    \item DBN (CUHK \cite{kang2013multi}).
    \item DNN (Google \cite{ze2013statistical}).
    \item DNN-GP (IBM \cite{fernandez2013f0}).
\end{itemize}

Поскольку уместить обзор всех существующих решений в рамках данной работы не получится, мы сфокусируемся на предыдущем решении, позиционирующим себя как state of the art. Покажем преимущества нашего подхода и обоснуем свой выбор.

\subsubsection{Deep Speech}

\begin{itemize}
    \item Использует рекуррентные нейронные сети для предсказания символьных последовательностей и потом применяет языковую модель.
    \item Использует 5 миллиардов связей для фразы \verb|(utterance, x_t)| средней длины.
    \item Каждый GPU считывает порции фраз одинаковой или почти одинаковой длины.
    \item Для рекуррентных слоев один GPU идет слева-направо, другой справа-налево и посередине они обмениваются коэффициентами и меняются ролями.
    \item 5000 часов речи от 9600 людей перемешали с различным шумом и между собой и получили 100 000 часов данных для обучения.
\end{itemize}
\cite{article:hannun2014deep}


\subsection{Сравнение с существующим решением}

Но даже Deep Speech не подходит для нашей задачи сразу по ряду причин:
% Почему вы выбрали Wavenet, а не какую-то из них.
\begin{enumerate}
    \item Вычислительная сложность.
\end{enumerate}

Как мы видим из описания Deep Speech рекуррентная сеть, главная особенность которой эффективное использование многих GPU, что требует огромных вычислительных ресурсов, которых у нас не было. 
\begin{enumerate}[resume]
    \item Естественность голоса.
\end{enumerate}
Ключевой заявленной сильной стороной Wavenet является именно естественность генерируемого голоса. Авторы даже ввели специальную метрику, основанную на мнении независимых слушателей. Учитывая, что мы хотим генерировать особенные голоса, для нас такая черта подлежащей модели достаточно важна.

\begin{enumerate}[resume]
    \item Нет инструментария для генерации с особенностями.
\end{enumerate}
Ну и самое главное: неясно, как в такой архитектуре можно реализовать генерацию с особенностями. 


% Вель фишка в том, что мы не обучвем на маелньких поддатасетах(что кстати работало бы хуже) а обучаем на всех и врубаем те услоия которые хотим. Как сделать такре без WaveNet не очень понятно.

% [ссыль на гуглопрезентацию гула]
% https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/41539.pdf

\end{document}

