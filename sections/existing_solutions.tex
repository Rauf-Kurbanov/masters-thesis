\documentclass[../diploma.tex]{subfiles}
 
\begin{document}
\label{sec:existing_solutions}
   

Методология генерации голоса имеет долгую историю, большую часть которой в основе самых прогрессивных методов лежали скрытые марковские модели.
Во многом благодаря тому, что стоит задача построение генеративной модели, марковские свойства до сих пристуствуют, однако могут полслабляться, как, нарпимер, в нашем решении. 
Сейчас же системы генерации голоса всё больше основываются на глубииное обучение как в качестве вспомогательного механизма, так и полной замены предыдущего подхода.

Генерация речи это уже довольно давно сформировавшаяся облась с своими нуюансами и богатой методологией.
Уместить обзор всех существующих решений у нас не получится, поэтому предлагаю сфокусироваться на решениях наиболее похожих на выбранное нами и показать, что я тут не изобретаю велосипед.

В последнее время системы на основе глубинного обучения захватили сферу генерации голоса, благодаря ряду преищуществ.

\subsection{Преимущетсва подходов на осонове глубинного обучения}

\begin{itemize}
    \item Встроенное извлечение признков
\end{itemize}    

Трудно спросить что что автоматическое или хотя бы частично-автоматическое извлечение признаков просто мечта для исследователя. Особенно при работе с такими сложными распределениями как изображения и звук, когда внутреннее устройство признаков в значительной мере предтсавляет из себя чёрный ящик.

\begin{itemize}
    \begin{itemize}
        \item Возможность эффективно моделировать сильно корреллирующие признаки большой размерности 
        \item Слоёная архитектура с нелинейными операторами позволяет беслпатно интегрировать извлечение признков в языковую модель
    \end{itemize}    
\end{itemize}

\begin{itemize}[resume]
    \item Распределённое представление
\end{itemize}    
    Здесь имеется ввиду, что нейросети зачастую способные единовременно захватывать большой участок пространства данных, когда какие-нибудь деревья решений дробят входное пространсво на области, что приводит к высокой фрагментации. 

\begin{itemize}
    \begin{itemize}
        \item Может быть экспоненциально более ээффективно чем фрагментированное представление
        \item Лучшие описательные качества с меньшим количеством параметров
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item Слоёная иерархическая структура системы генерации речи
\end{itemize}

Глубокие нейросети неявно расслаивают промежуточные предстваление, при движении вглубь по слоям.

Звуковая волна $\rightarrow$ концептуальное предтсавление $\rightarrow$ лингвистическое $\rightarrow$ представление с упором на описание методов произношения $\rightarrow$ звуковая волна
\cite{pres:google_dl_speech}.


% \subsection{Лучшие системы на сегодняшний день}

% \subsection{Deep Belief Network}
% [мб картинка]
% of multiple layers of stochastic latent variables with Restricted Boltzmann Machines (RBMs) as their building blocks. DBNs have a greedy layer-wise unsupervised learning algorithm as well as a discriminative finetuning procedure for optimizing performance on classification tasks

Хоть целью работы и не является генерация речи напрямую, может возникнуть вопрос, почему была выбрана та ли иная подлежащая нейросетевая архитектура. Благо создатели нейросетевой архитектуры WaveNet  позиционируют её как новый "state of the art", поэтмоу с любыми вопросами относительно качества генерации мы можем отправить в оригинальную статью \cite{article:van2016wavenet}.

Однако, качетсво генерации было не единственным критерием выбора, поэтому мы опишем предыдущую "state of the art" систему и обоснуем свой выбор. 

\subsection{Deep Speech}
\begin{itemize}
    \item Использует рекуррентные нейронные сети для предсказания символьных последовательностей и потом применяет языковую модель
    \item использует 5 миллиардов связей для фразы \verb|(utterance, x_t)| средней длины
    \item Каждый GPU считвыает порции фраз одинаковой или почти одинаковой длины
    \item Для рекуррентных слоев один GPU идет слева-направо, другой справа-налево и посередине они обмениваются коэффициентами и меняются ролями.
    \item 5000 часов речи от 9600 людей перемешали с различным шумом и между собой и получили 100 000 часов данных для обучения
\end{itemize}
\cite{article:hannun2014deep}


\subsection{Сравнение с существующим решением}

Deep Speech не подходит для нашей задачи сразу по ряду причин.
% Почему вы выбрали Wavenet, а не какую-то из них.
\begin{enumerate}
    \item Вычислительная сложность
\end{enumerate}
Как мы видим из описания Deep Speech рекуррентная сеть, главная особенность которой эффектривное использование многих GPU, у нас не было даже близко таких вычислительных ресурсов 
\begin{enumerate}[resume]
    \item Естественность голоса
\end{enumerate}
Главной заявленной сильной стороной Wavenet язвляется именно естественность генериреумого голоса. Авторы даже ввели специальную метрику, основанную на мнении независимых слушателей. Учитывая, что мы хотим негерировать какие-то особенные голоса, для нас такая черта подлежащей модели достаточно важна.

\begin{enumerate}[resume]
    \item Нет инструментария для генерации с особенностями
\end{enumerate}
Ну и самое главное: неясно, как в такой архитектуре можно реализовать генерацию с особенностями. 


% Вель фишка в том, что мы не обучвем на маелньких поддатасетах(что кстати работало бы хуже) а обучаем на всех и врубаем те услоия которые хотим. Как сделать такре без WaveNet не очень понятно.

% [ссыль на гуглопрезентацию гула]
% https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/41539.pdf

\end{document}

